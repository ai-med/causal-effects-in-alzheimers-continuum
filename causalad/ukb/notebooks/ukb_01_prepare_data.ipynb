{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This file is part of Estimation of Causal Effects in the Alzheimer's Continuum (Causal-AD).\n",
    "\n",
    "Causal-AD is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "Causal-AD is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with Causal-AD. If not, see <https://www.gnu.org/licenses/>.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare UKB Data\n",
    "\n",
    "- Load volume and thickness measurements\n",
    "- Merge measurements of certain areas\n",
    "- Divide volumes by TIV\n",
    "- Apply Cox-Box transform to each measurement\n",
    "- Standardize each measurement to zero mean and unti variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from causalad.ukb import data\n",
    "from causalad.ukb.estimate import fit_regress_out\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "\n",
    "csv_file: str = \"ukb-data.csv\"\n",
    "output_dir: str = \".\"\n",
    "num_sites: int = 3\n",
    "seed: int = 21012171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data.UKBDataLoader(csv_file, drop_outliers=False)\n",
    "vols, thicks, demos = loader.load_freesurfer()\n",
    "\n",
    "vols.shape, thicks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FreeSurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat = vols.corr(method=\"spearman\")\n",
    "cor_mat.values[np.diag_indices_from(cor_mat)] = 0.0\n",
    "sns.clustermap(\n",
    "    cor_mat, method=\"ward\", metric=\"euclidean\", annot=True, figsize=(12, 12), cmap=\"RdBu_r\",\n",
    ")\n",
    "\n",
    "del cor_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://radiopaedia.org/articles/cingulate-gyrus\n",
    "- http://braininfo.rprc.washington.edu/centraldirectory.aspx?ID=159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobes_map = data.get_lobes_map(thicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat = thicks.corr(method=\"spearman\")\n",
    "cor_mat.values[np.diag_indices_from(cor_mat)] = 0.0\n",
    "sns.clustermap(cor_mat, method=\"ward\", metric=\"euclidean\",\n",
    "               row_cluster=False,\n",
    "#                col_colors=lobes_map.loc[:, \"color\"],\n",
    "               square=True, annot=True, figsize=(19, 19), cmap=\"RdBu_r\")\n",
    "\n",
    "del cor_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune redundant measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thicks_pruned = data.prune_by_group(thicks, lobes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat = thicks_pruned.corr(method=\"spearman\")\n",
    "cor_mat.values[np.diag_indices_from(cor_mat)] = 0.0\n",
    "sns.clustermap(cor_mat, method=\"ward\", metric=\"euclidean\",\n",
    "               row_cluster=False,\n",
    "#                col_colors=lobes_map.loc[:, \"color\"],\n",
    "               square=True, annot=True, figsize=(19, 19), cmap=\"RdBu_r\")\n",
    "\n",
    "del cor_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Volume and Thickness Measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_data = pd.concat((vols, thicks_pruned), axis=1)\n",
    "\n",
    "ukb_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data\n",
    "\n",
    "Such that it is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality_check(data):\n",
    "    n_features = data.shape[1]\n",
    "    n_cols = 5\n",
    "    n_rows = int(np.ceil(n_features / n_cols))\n",
    "    _, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 3,  n_rows * 3),\n",
    "                          sharex=True, sharey=True)\n",
    "    for (a, b), ax in zip(data.iteritems(), axs.flat):\n",
    "        stats.probplot(b, plot=ax)\n",
    "        ax.set_title(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize volumes by dividing by eTIV\n",
    "tiv = ukb_data.loc[:, \"eTIV\"]\n",
    "ukb_data_t = ukb_data.drop(\"eTIV\", axis=1)\n",
    "vols_mask = ~ukb_data_t.columns.str.endswith(\"_thickness\")\n",
    "ukb_data_t.loc[:, vols_mask] = ukb_data_t.loc[:, vols_mask].div(tiv, axis=0)\n",
    "\n",
    "del vols_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_data_t, ukb_transforms = data.apply_transform(ukb_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normality_check(ukb_data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unobserved Confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unobserved_confounder(\n",
    "    vol_thick_data: pd.DataFrame, obs_conv: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # regress-out observed confounder\n",
    "    causal_data = fit_regress_out(vol_thick_data, obs_conv)\n",
    "\n",
    "    p = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate=10.0,\n",
    "        perplexity=30,\n",
    "        init=\"pca\",\n",
    "        random_state=seed,\n",
    "    ).fit(causal_data.values)\n",
    "    Xtc = MinMaxScaler().fit_transform(p.embedding_)\n",
    "\n",
    "    km = KMeans(\n",
    "        n_clusters=num_sites,\n",
    "        init=\"k-means++\",\n",
    "        n_init=10,\n",
    "        max_iter=1000,\n",
    "        tol=1e-6,\n",
    "        algorithm=\"full\",\n",
    "        random_state=seed,\n",
    "    ).fit(Xtc)\n",
    "    site_id = km.predict(Xtc)  # cluster by all features\n",
    "    intercepts = np.arange(1, num_sites + 1)\n",
    "\n",
    "    unobs_conf = pd.DataFrame(\n",
    "        intercepts[site_id], index=vol_thick_data.index, columns=[\"unobserved_confounder\"],\n",
    "    )\n",
    "\n",
    "    Xt = pd.DataFrame(Xtc, columns=[\"Dim1\", \"Dim2\"])\n",
    "    Xt.loc[:, \"site\"] = [f\"S{i}\" for i in site_id]\n",
    "    sns.jointplot(data=Xt, x=Xt.columns[0], y=Xt.columns[1], hue=\"site\", kind=\"kde\")\n",
    "\n",
    "    return unobs_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_unobs = generate_unobserved_confounder(ukb_data_t, data.get_volume_causes(demos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_unobs.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data, tiv, transforms, confounders, filename):\n",
    "    with pd.HDFStore(filename, complib=\"lzo\") as store:\n",
    "        thicks = data.loc[:, data.columns.str.endswith(\"_thickness\")]\n",
    "        vols = data.drop(thicks.columns, axis=1)\n",
    "\n",
    "        store.put(\"volumes\", vols)\n",
    "        store.put(\"thickness\", thicks)\n",
    "        store.put(\"tiv\", pd.DataFrame(tiv))\n",
    "        store.put(\"demographics\", demos)\n",
    "        store.put(\"transforms\", transforms)\n",
    "        store.put(\"confounders\", confounders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\n",
    "    ukb_data_t,\n",
    "    tiv,\n",
    "    ukb_transforms,\n",
    "    conf_unobs,\n",
    "    Path(output_dir) / \"ukb_data_t.h5\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
